{
  "uniform_median": {
    "variant": "uniform_median",
    "suggested_r": 16,
    "actual_r": 16,
    "rank_pattern": {},
    "alpha_pattern": {},
    "config": {
      "probe_r": 16,
      "alpha": 16,
      "dropout": 0.0,
      "target_modules": [
        "q_lin",
        "k_lin",
        "v_lin",
        "out_lin"
      ]
    },
    "status": "ready",
    "reason": null
  },
  "uniform_p90": {
    "variant": "uniform_p90",
    "suggested_r": 16,
    "actual_r": 16,
    "rank_pattern": {},
    "alpha_pattern": {},
    "config": {
      "probe_r": 16,
      "alpha": 16,
      "dropout": 0.0,
      "target_modules": [
        "q_lin",
        "k_lin",
        "v_lin",
        "out_lin"
      ]
    },
    "status": "ready",
    "reason": null
  },
  "per_layer": {
    "variant": "per_layer",
    "suggested_r": 9,
    "actual_r": 9,
    "rank_pattern": {
      "base_model.model.distilbert.transformer.layer.1.attention.v_lin": 8,
      "base_model.model.distilbert.transformer.layer.2.attention.v_lin": 4,
      "base_model.model.distilbert.transformer.layer.3.attention.v_lin": 8,
      "base_model.model.distilbert.transformer.layer.4.attention.out_lin": 8,
      "base_model.model.distilbert.transformer.layer.4.attention.q_lin": 8,
      "base_model.model.distilbert.transformer.layer.4.attention.v_lin": 4,
      "base_model.model.distilbert.transformer.layer.5.attention.out_lin": 8,
      "base_model.model.distilbert.transformer.layer.5.attention.q_lin": 4,
      "base_model.model.distilbert.transformer.layer.5.attention.v_lin": 4
    },
    "alpha_pattern": {
      "base_model.model.distilbert.transformer.layer.1.attention.v_lin": 8,
      "base_model.model.distilbert.transformer.layer.2.attention.v_lin": 4,
      "base_model.model.distilbert.transformer.layer.3.attention.v_lin": 8,
      "base_model.model.distilbert.transformer.layer.4.attention.out_lin": 8,
      "base_model.model.distilbert.transformer.layer.4.attention.q_lin": 8,
      "base_model.model.distilbert.transformer.layer.4.attention.v_lin": 4,
      "base_model.model.distilbert.transformer.layer.5.attention.out_lin": 8,
      "base_model.model.distilbert.transformer.layer.5.attention.q_lin": 4,
      "base_model.model.distilbert.transformer.layer.5.attention.v_lin": 4
    },
    "config": {
      "probe_r": null,
      "alpha": null,
      "dropout": 0.0,
      "target_modules": [
        "q_lin",
        "k_lin",
        "v_lin",
        "out_lin"
      ],
      "rank_pattern": {
        "base_model.model.distilbert.transformer.layer.1.attention.v_lin": 8,
        "base_model.model.distilbert.transformer.layer.2.attention.v_lin": 4,
        "base_model.model.distilbert.transformer.layer.3.attention.v_lin": 8,
        "base_model.model.distilbert.transformer.layer.4.attention.out_lin": 8,
        "base_model.model.distilbert.transformer.layer.4.attention.q_lin": 8,
        "base_model.model.distilbert.transformer.layer.4.attention.v_lin": 4,
        "base_model.model.distilbert.transformer.layer.5.attention.out_lin": 8,
        "base_model.model.distilbert.transformer.layer.5.attention.q_lin": 4,
        "base_model.model.distilbert.transformer.layer.5.attention.v_lin": 4
      },
      "alpha_pattern": {
        "base_model.model.distilbert.transformer.layer.1.attention.v_lin": 8,
        "base_model.model.distilbert.transformer.layer.2.attention.v_lin": 4,
        "base_model.model.distilbert.transformer.layer.3.attention.v_lin": 8,
        "base_model.model.distilbert.transformer.layer.4.attention.out_lin": 8,
        "base_model.model.distilbert.transformer.layer.4.attention.q_lin": 8,
        "base_model.model.distilbert.transformer.layer.4.attention.v_lin": 4,
        "base_model.model.distilbert.transformer.layer.5.attention.out_lin": 8,
        "base_model.model.distilbert.transformer.layer.5.attention.q_lin": 4,
        "base_model.model.distilbert.transformer.layer.5.attention.v_lin": 4
      }
    },
    "status": "ready",
    "reason": null
  }
}