
================================================================================
MAJOR FINDING: ARCHITECTURE DETERMINES GROKKING DYNAMICS
================================================================================

EXPERIMENT: Modular multiplication (p=97), same hyperparameters, 5 seeds each

                        TRANSFORMER          MLP
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Grokking rate           5/5 (100%)           0/5 (0%)*
Steps to grok           7k-13k (mean 9.7k)   33.5k (single run)
Rank compression        ~14%                 ~67%
Direction changes       0 (all seeds)        2+ 
Trajectory              Perfectly monotonic  Occasional oscillation
Test acc @ step 6000    92.7%                1.8%

*Original MLP run with different seed grokked at 33.5k

================================================================================
INTERPRETATION
================================================================================

1. ATTENTION AS INDUCTIVE BIAS
   - Transformers "know" how to route information between tokens
   - MLPs must discover this routing through weight structure
   - This manifests as less representational compression needed

2. TRAJECTORY QUALITY
   - Transformer: Perfectly smooth rank decay (0 oscillations)
   - MLP: Occasional reversals indicating "search" behavior
   - Smooth trajectory = architecture matches task structure

3. EARLY SIGNAL (step 6000)
   - Transformer: 92.7% accuracy
   - MLP: 1.8% accuracy
   - Outcome predictable VERY early

================================================================================
IMPLICATIONS FOR GRADIENCE
================================================================================

DIAGNOSTIC: Trajectory smoothness as architecture-task fit indicator
- 0 direction changes = architecture well-suited to task
- Multiple reversals = architecture struggling, may need more time/capacity

EARLY STOPPING REFINEMENT:
- For transformers on algorithmic tasks: If not >50% by 5k steps, investigate
- For MLPs: More patience needed, watch for oscillation

================================================================================
THEORETICAL CONNECTION
================================================================================

This connects to the "lottery ticket" and "inductive bias" literature:
- Transformers have "winning tickets" for sequence tasks built-in
- MLPs must find them through training
- Spectral compression measures this search process

================================================================================
