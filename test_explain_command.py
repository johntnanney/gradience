#!/usr/bin/env python3
"""
Test the 'explain' command for layer-specific disagreement analysis.

Creates a sample audit JSON and demonstrates the explain command's
ability to break down exactly why a layer was/wasn't flagged.
"""

import sys
import json
import tempfile
import os
sys.path.insert(0, '.')

from gradience.cli import _analyze_policy_disagreements

def create_sample_audit_json():
    """Create a sample audit JSON with disagreement analysis for testing."""
    
    class MockLayer:
        def __init__(self, name, rank_suggestions, frob_sq=1.0, params=1000, utilization=0.5):
            self.name = name
            self.rank_suggestions = rank_suggestions
            self.frob_sq = frob_sq
            self.params = params
            self.utilization = utilization
    
    name_mapping = {
        'energy_threshold': 'energy@0.90',
        'knee_elbow': 'knee',
        'entropy_effective': 'erank'
    }
    
    # Create a realistic scenario for explanation
    layers = [
        # High-impact layer (should be flagged) - great example for explanation
        MockLayer("model.layers.0.self_attn.q_proj", {
            'energy_threshold': {'k': 8, 'confidence': 0.90},
            'knee_elbow': {'k': 2, 'confidence': 0.85},
            'entropy_effective': {'k': 6, 'confidence': 0.90}
        }, frob_sq=50.0, params=50000, utilization=0.8),
        
        # Medium-impact layer (not flagged due to uniform_mult) - good failure example
        MockLayer("model.layers.1.mlp.up_proj", {
            'energy_threshold': {'k': 6, 'confidence': 0.90},
            'knee_elbow': {'k': 3, 'confidence': 0.85},
            'entropy_effective': {'k': 5, 'confidence': 0.90}
        }, frob_sq=20.0, params=30000, utilization=0.6),
        
        # Low-impact layer (not flagged due to importance) - clear failure case
        MockLayer("model.layers.2.norm", {
            'energy_threshold': {'k': 4, 'confidence': 0.90},
            'knee_elbow': {'k': 1, 'confidence': 0.85},
            'entropy_effective': {'k': 3, 'confidence': 0.90}
        }, frob_sq=2.0, params=1000, utilization=0.2),
        
        # Consensus layer (no disagreement) - different failure case
        MockLayer("model.layers.3.ln_final", {
            'energy_threshold': {'k': 4, 'confidence': 0.90},
            'knee_elbow': {'k': 4, 'confidence': 0.85},
            'entropy_effective': {'k': 4, 'confidence': 0.90}
        }, frob_sq=1.0, params=500, utilization=0.1)
    ]
    
    # Generate analysis
    analysis = _analyze_policy_disagreements(layers, name_mapping, None, "full")
    
    # Create sample audit JSON structure
    audit_json = {
        "schema_version": 1,
        "computed_at": "2026-01-28T17:20:00Z",
        "policy_disagreement_analysis": analysis,
        "adapter_info": {
            "total_layers": len(layers),
            "model_name": "example_model_for_explain_demo"
        }
    }
    
    return audit_json


def test_explain_command():
    """Test the explain command with various layer scenarios."""
    
    print("üìñ TESTING EXPLAIN COMMAND")
    print("=" * 80)
    
    # Create sample audit JSON
    audit_data = create_sample_audit_json()
    
    # Save to temporary file
    with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
        json.dump(audit_data, f, indent=2)
        audit_file = f.name
    
    print(f"Created sample audit JSON: {audit_file}")
    
    try:
        # Show available layers first
        analysis = audit_data["policy_disagreement_analysis"]
        flagged_layers = [l["layer_name"] for l in analysis["flagged_layers"]]
        all_layers = [l["layer_name"] for l in analysis["all_layers_with_disagreement"]]
        
        print(f"\nAvailable layers for explanation:")
        for layer in all_layers:
            status = "üî• FLAGGED" if layer in flagged_layers else "‚óã not flagged"
            print(f"  {layer} {status}")
        
        print(f"\nüéØ Example explain commands you can run:")
        print(f"=" * 60)
        
        # High-impact layer example
        if flagged_layers:
            print(f"# Explain a HIGH-IMPACT layer:")
            print(f"python -m gradience.cli explain \\")
            print(f"  --audit-json {audit_file} \\")
            print(f"  --layer '{flagged_layers[0]}'")
            print(f"# Shows: ‚úÖ why it was flagged, threshold analysis, recommendations")
        
        # Non-flagged layer example  
        non_flagged = [l for l in all_layers if l not in flagged_layers]
        if non_flagged:
            print(f"\\n# Explain a NON-FLAGGED layer:")
            print(f"python -m gradience.cli explain \\")
            print(f"  --audit-json {audit_file} \\")
            print(f"  --layer '{non_flagged[0]}'")
            print(f"# Shows: ‚ùå why it failed, which thresholds it missed")
        
        # Verbose mode
        print(f"\\n# Verbose mode (detailed thresholds):")
        print(f"python -m gradience.cli explain \\")
        print(f"  --audit-json {audit_file} \\")
        print(f"  --layer '{all_layers[0]}' \\")
        print(f"  --verbose")
        
        # Error case
        print(f"\\n# Error handling (invalid layer):")
        print(f"python -m gradience.cli explain \\")
        print(f"  --audit-json {audit_file} \\")
        print(f"  --layer 'nonexistent.layer'")
        print(f"# Shows: Available layers list")
        
        # Demonstrate the actual functionality
        print(f"\\n" + "=" * 80)
        print(f"üöÄ LIVE DEMONSTRATION")
        print(f"=" * 80)
        
        # Import and use the explain functionality directly
        from gradience.cli import cmd_explain
        import argparse
        
        # Simulate command line args for a flagged layer
        if flagged_layers:
            print(f"\\nüî• EXPLAINING FLAGGED LAYER: {flagged_layers[0]}")
            print("-" * 60)
            
            # Create mock args
            args = argparse.Namespace()
            args.audit_json = audit_file
            args.layer = flagged_layers[0]
            args.verbose = False
            
            # Run explain
            cmd_explain(args)
        
        # Simulate for a non-flagged layer
        if non_flagged:
            print(f"\\n\\n‚óã EXPLAINING NON-FLAGGED LAYER: {non_flagged[0]}")
            print("-" * 60)
            
            args = argparse.Namespace()
            args.audit_json = audit_file
            args.layer = non_flagged[0]
            args.verbose = False
            
            cmd_explain(args)
        
        # Show benefits
        print(f"\\n\\n‚úÖ EXPLAIN COMMAND BENEFITS")
        print("=" * 80)
        print("üéØ Prevents JSON spelunking:")
        print("  ‚Ä¢ No need to manually parse complex JSON structure")
        print("  ‚Ä¢ Clear, human-readable threshold breakdown")
        print("  ‚Ä¢ Specific recommendations for each layer")
        
        print("\\n‚ö° Instant insights:")
        print("  ‚Ä¢ Why was this layer flagged/not flagged?")
        print("  ‚Ä¢ Which exact thresholds passed/failed?")
        print("  ‚Ä¢ What should I do with this layer?")
        
        print("\\nüîç Perfect for debugging:")
        print("  ‚Ä¢ Understand disagreement analysis decisions")
        print("  ‚Ä¢ Verify threshold calculations")
        print("  ‚Ä¢ Get policy-specific rank suggestions")
        
        print("\\nüöÄ Use cases:")
        print("  ‚Ä¢ 'Why is this layer high-priority for Bench?'")
        print("  ‚Ä¢ 'Should I use per-layer optimization here?'")
        print("  ‚Ä¢ 'What rank should I assign to this layer?'")
        print("  ‚Ä¢ 'Why did my layer not get flagged?'")
        
        return audit_file
        
    except Exception as e:
        print(f"Error during test: {e}")
        return None


def demonstrate_edge_cases(audit_file):
    """Demonstrate edge cases and error handling."""
    
    if not audit_file:
        return
        
    print(f"\\n\\nüß™ EDGE CASES & ERROR HANDLING")
    print("=" * 80)
    
    from gradience.cli import cmd_explain
    import argparse
    
    print("\\n1Ô∏è‚É£ Invalid layer name:")
    print("-" * 40)
    try:
        args = argparse.Namespace()
        args.audit_json = audit_file
        args.layer = "nonexistent.layer.name"
        args.verbose = False
        
        cmd_explain(args)
    except SystemExit:
        print("‚úì Correctly handled invalid layer name with helpful error")
    
    print("\\n2Ô∏è‚É£ Missing audit file:")
    print("-" * 40)
    try:
        args = argparse.Namespace()
        args.audit_json = "/nonexistent/audit.json"
        args.layer = "any.layer"
        args.verbose = False
        
        cmd_explain(args)
    except SystemExit:
        print("‚úì Correctly handled missing audit file")
    
    print("\\n3Ô∏è‚É£ Verbose mode demonstration:")
    print("-" * 40)
    print("(Would show detailed threshold descriptions and distribution info)")
    
    # Cleanup
    try:
        os.unlink(audit_file)
        print(f"\\nüßπ Cleaned up temporary file: {audit_file}")
    except:
        pass


if __name__ == '__main__':
    print("üìñ EXPLAIN COMMAND TEST & DEMONSTRATION")
    print("=" * 90)
    print("Testing the new 'gradience explain' command that prevents JSON spelunking")
    print("and provides instant layer-specific disagreement analysis.")
    print("=" * 90)
    
    # Main test
    audit_file = test_explain_command()
    
    # Edge cases
    demonstrate_edge_cases(audit_file)
    
    print("\\n" + "=" * 90)
    print("‚úÖ EXPLAIN COMMAND IMPLEMENTATION COMPLETE!")
    print()
    print("üéØ Usage:")
    print("  gradience explain --audit-json <path> --layer <layer_name> [--verbose]")
    print()
    print("üí° Perfect for:")
    print("  ‚Ä¢ Understanding why specific layers were/weren't flagged")
    print("  ‚Ä¢ Getting exact threshold values and calculations")
    print("  ‚Ä¢ Receiving targeted recommendations for each layer")
    print("  ‚Ä¢ Debugging disagreement analysis decisions")
    print()
    print("This eliminates the need for manual JSON parsing and provides")
    print("instant, actionable insights for any layer! üöÄ")